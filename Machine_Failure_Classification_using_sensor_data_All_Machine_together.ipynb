{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8nkaCZNq4uWn"
   },
   "source": [
    "# Machine Failure Classification using sensor data\n",
    "\n",
    "By: Nathália Alvim\n",
    "\n",
    "E-mail: natalvimdesouza@hotmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZjWxSUjgEqk"
   },
   "source": [
    "Dataset is avaible on kaggle.\n",
    "\n",
    "https://www.kaggle.com/datasets/mujtabamatin/dataset-for-machine-failure-detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "draaPnOZfFHh"
   },
   "source": [
    "The dataset has 1000 samples and 6 variables.\n",
    "\n",
    "\n",
    "The dataset consists of the following features:\n",
    "\n",
    "- Temperature (°C): Continuous data representing the temperature at the machine's\n",
    "critical points. Higher temperatures may indicate potential issues due to overheating.\n",
    "\n",
    "- Vibration (Hz): Frequency of machine vibrations. Abnormal vibrations can signal mechanical misalignment, imbalance, or wear.\n",
    "\n",
    "- Power Usage (kW): Power consumption levels of the machine. Spikes in power usage may indicate increased load or potential mechanical issues.\n",
    "\n",
    "- Humidity (%): Environmental humidity around the machine. High humidity levels could affect machine performance and lead to failure over time.\n",
    "\n",
    "- Machine Type: Categorical data indicating the type of machine (e.g., \"Drill\", \"Lathe\", \"Mill\"). Different machine types may have unique failure patterns.\n",
    "\n",
    "Target Variable:\n",
    "\n",
    "- Failure Risk: A binary label where 0 indicates normal operation, and 1 indicates that the machine is at risk of failure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installing Lazy Predict\n",
    "!pip install lazypredict\n",
    "!pip install tensorflow\n",
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k1hDRA9n4r3H"
   },
   "source": [
    "##Importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Phz1OGAqe3-3"
   },
   "outputs": [],
   "source": [
    "#Import packages for pre processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import shapiro, normaltest, anderson\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, cross_validate\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "from sklearn.metrics import auc, RocCurveDisplay, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score, recall_score, precision_score\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "#Import packages tensor flow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras import backend as K\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "rQX_x1NPgzCE",
    "outputId": "e84d98d5-6a14-49f6-d550-5a2a2d3b077b"
   },
   "outputs": [],
   "source": [
    "#Loading the dataset\n",
    "file_path = '/workspaces/Machine-Learning/machine_failure_dataset.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KJmcjvEVCRTJ",
    "outputId": "d3fbd8f0-75e2-4936-b6b8-d1a05d800e70"
   },
   "outputs": [],
   "source": [
    "# General information\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JUi7KQ5w4-3S"
   },
   "source": [
    "## Class Balacing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zLylVDFWi55S",
    "outputId": "b10ca347-2ea9-4152-b976-a1e435db35fb"
   },
   "outputs": [],
   "source": [
    "# Class Balancing\n",
    "# First i'll apply for the categorical variables and after i'll do with the binary variable\n",
    "\n",
    "categorical = 'Machine_Type'\n",
    "\n",
    "#Available categories\n",
    "\n",
    "categories = df[categorical].unique()\n",
    "\n",
    "#Counting\n",
    "counting = df[categorical].value_counts()\n",
    "\n",
    "print(categories)\n",
    "print(counting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Jr-Vjii3dSx",
    "outputId": "84181e7c-aa1f-45c7-b3ce-0c3eeec9bee9"
   },
   "outputs": [],
   "source": [
    "#Binary variable\n",
    "\n",
    "binary = 'Failure_Risk'\n",
    "\n",
    "#Available categories\n",
    "\n",
    "categories = df[binary].unique() # 0 = Normal operation \\\\ 1 = Risk of failure\n",
    "\n",
    "#Counting\n",
    "counting = df[binary].value_counts()\n",
    "\n",
    "print(categories)\n",
    "print(counting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJSNeslp4pim"
   },
   "source": [
    "## Descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fsM0SFPy4ZRO",
    "outputId": "ced4f676-6896-43ce-f378-56b6a8b9e744"
   },
   "outputs": [],
   "source": [
    "# Calculate mean, median, and mode for each numerical column\n",
    "# Select only numerical columns (nc) and exclude the binary variable\n",
    "# Select only numerical columns and exclude a specific one\n",
    "nc = df.select_dtypes(include=['number']).drop(columns=['Failure_Risk'])\n",
    "\n",
    "mean = nc.mean()\n",
    "median = nc.median()\n",
    "mode = nc.mode().iloc[0]  # Takes the first mode found for each column\n",
    "std = nc.std()  # Standard deviation\n",
    "cv = std / mean  # Coefficient of Variation\n",
    "\n",
    "\n",
    "print(\"\\nMean of each column:\")\n",
    "print(mean)\n",
    "\n",
    "print(\"\\nMedian of each column:\")\n",
    "print(median)\n",
    "\n",
    "print(\"\\nMode of each column:\")\n",
    "print(mode)\n",
    "\n",
    "print(\"\\nStandard Deviation of each column:\")\n",
    "print(std)\n",
    "\n",
    "print(\"\\nCoefficient of Variation (CV) of each column:\")\n",
    "print(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "ckl6Jl3K59nB",
    "outputId": "3aad9145-6722-4a3d-8f20-a196a7b6ad04"
   },
   "outputs": [],
   "source": [
    "# Boxpot\n",
    "# Generate boxplot to visualize distribution and possible outliers\n",
    "plt.figure(figsize=(10, 6))\n",
    "nc.boxplot()\n",
    "plt.title(\"Dataset Boxplot\")\n",
    "plt.xlabel(\"Columns\")\n",
    "plt.ylabel(\"Values\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "OPHOKN5m6fTC",
    "outputId": "94537cfa-1be5-433c-c461-0d94b244f5f2"
   },
   "outputs": [],
   "source": [
    "# Generate paired boxplots to save space and for better visualization\n",
    "columns = nc.columns\n",
    "num_columns = len(columns)\n",
    "\n",
    "# Iterate over the columns in pairs\n",
    "for i in range(0, num_columns, 2):\n",
    "    # Set up the subplot for 2 side-by-side plots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # First boxplot\n",
    "    axes[0].boxplot(nc[columns[i]].dropna(), vert=True, patch_artist=True)\n",
    "    axes[0].set_title(f\"Boxplot - {columns[i]}\")\n",
    "    axes[0].set_xlabel(columns[i])\n",
    "    axes[0].set_ylabel(\"Values\")\n",
    "\n",
    "    # Second boxplot (if it exists)\n",
    "    if i + 1 < num_columns:\n",
    "        axes[1].boxplot(nc[columns[i + 1]].dropna(), vert=True, patch_artist=True)\n",
    "        axes[1].set_title(f\"Boxplot - {columns[i + 1]}\")\n",
    "        axes[1].set_xlabel(columns[i + 1])\n",
    "        axes[1].set_ylabel(\"Values\")\n",
    "    else:\n",
    "        # Remove the second subplot if there's no second column\n",
    "        fig.delaxes(axes[1])\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYPvGNHT79W_"
   },
   "source": [
    "## Normality of the dataset\n",
    "\n",
    "- Shapiro-Wilk\n",
    "\n",
    "- D’Agostino-Pearson\n",
    "\n",
    "- Anderson-Darling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0YHEYnK2779j"
   },
   "outputs": [],
   "source": [
    "def normality_tests_summary(dataframe):\n",
    "    numeric_cols = dataframe.select_dtypes(include=['number'])\n",
    "    results = []\n",
    "\n",
    "    for col in numeric_cols.columns:\n",
    "        col_data = numeric_cols[col].dropna()\n",
    "\n",
    "        # Shapiro-Wilk Test\n",
    "        shapiro_stat, shapiro_p = shapiro(col_data)\n",
    "        shapiro_result = \"Normal\" if shapiro_p > 0.05 else \"Not normal\"\n",
    "\n",
    "        # D’Agostino and Pearson Test\n",
    "        dagostino_stat, dagostino_p = normaltest(col_data)\n",
    "        dagostino_result = \"Normal\" if dagostino_p > 0.05 else \"Not normal\"\n",
    "\n",
    "        # Anderson-Darling Test\n",
    "        anderson_result_obj = anderson(col_data)\n",
    "        ad_stat = anderson_result_obj.statistic\n",
    "        # Compare with 5% significance level\n",
    "        ad_result = \"Normal\" if ad_stat < anderson_result_obj.critical_values[2] else \"Not normal\"\n",
    "\n",
    "        results.append({\n",
    "            \"Column\": col,\n",
    "            \"Shapiro-Wilk (p)\": round(shapiro_p, 4),\n",
    "            \"Shapiro Result\": shapiro_result,\n",
    "            \"D’Agostino (p)\": round(dagostino_p, 4),\n",
    "            \"D’Agostino Result\": dagostino_result,\n",
    "            \"Anderson-Darling (stat)\": round(ad_stat, 4),\n",
    "            \"Anderson Result (5%)\": ad_result\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cf51DMkE8t69",
    "outputId": "eeebed66-5ec2-49fa-d1dc-667db0b212ce"
   },
   "outputs": [],
   "source": [
    "summary_df = normality_tests_summary(nc)\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NTAFCPGkDS2W"
   },
   "outputs": [],
   "source": [
    "#Convert categorical variable to 1, 2, 3 variable\n",
    "label_encoder = LabelEncoder()\n",
    "df['Machine_Type_encoded'] = label_encoder.fit_transform(df['Machine_Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kT3uoBkZOUi2",
    "outputId": "51799680-7a10-4b27-ee0a-c855693b25da"
   },
   "outputs": [],
   "source": [
    "for i, category in enumerate(label_encoder.classes_):\n",
    "    print(f\"{category} -> {i + 1}\")\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JYR4-0YtO_r6",
    "outputId": "47db0d99-41d7-480d-83dd-6e1c1582ca5a"
   },
   "outputs": [],
   "source": [
    "#Remove Machine_Type and renomed Machine_Type_encoded to Machine_Type\n",
    "df.drop('Machine_Type', axis=1, inplace=True)\n",
    "df.rename(columns={'Machine_Type_encoded': 'Machine_Type'}, inplace=True)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WuzKr0ZyHyYU",
    "outputId": "9d83226a-6dc2-4430-9559-2d1059c8db0c"
   },
   "outputs": [],
   "source": [
    "# Correlation with the Failure_Risk variable\n",
    "correlations = df.select_dtypes(include=['number']).corr()[\"Failure_Risk\"].drop(\"Failure_Risk\")\n",
    "print(correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFjtPiyU6tyr"
   },
   "source": [
    "## Pearson correlation map\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 699
    },
    "id": "rSlHbRGm6q1e",
    "outputId": "cf6675be-4827-457b-9ce1-e17d40a9329b"
   },
   "outputs": [],
   "source": [
    "# Pearson Correlation Map\n",
    "# Calculate the correlation matrix (Pearson coefficient)\n",
    "pearson_correlation = nc.corr(method='pearson')\n",
    "\n",
    "# Generate the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(pearson_correlation, annot=True, cmap=\"coolwarm\", fmt=\".2f\", cbar=True, square=True)\n",
    "plt.title(\"Correlation Map (Pearson)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pwMTXoPxJebh"
   },
   "source": [
    "### Conclusion:\n",
    "\n",
    "The data set follows a normal distribution by tests performed such as Shapiro Wilk.\n",
    "A graph was generated in order to define the linearity of the system, thus concluding that the system is non-linear.\n",
    "Finally, the Pearson correlation map was created to see the relationship between the variables.\n",
    "\n",
    "In summary:\n",
    "\n",
    "-> Non-linear\n",
    "\n",
    "-> Normal Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IB2_i6-ODTZc"
   },
   "source": [
    "## Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "id": "E0E2q_ySDWg8",
    "outputId": "66e08951-a507-4c6e-b731-2b8f4077f63c"
   },
   "outputs": [],
   "source": [
    "# Separate features (X) and target (y)\n",
    "X = df.drop(\"Failure_Risk\", axis=1)\n",
    "y = df[\"Failure_Risk\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WCtFpSItFxSh"
   },
   "outputs": [],
   "source": [
    "#80 training and 20 for test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qfYrHYptGC-N",
    "outputId": "f6d0e2d4-d7d3-4135-a23c-a87508b7045c"
   },
   "outputs": [],
   "source": [
    "#Scale numerical features\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(X_test_scaled)\n",
    "print(X_train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-jMKoWdOPeqb"
   },
   "source": [
    "I normalized everything, so if the Machine Type was 1, now is 0, if it was 2 now is 0.5 and if it was 3 now is 1.\n",
    "\n",
    "Drill -> 0\n",
    "\n",
    "Lathe -> 0.5\n",
    "\n",
    "Mill -> 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "t4v6jMfBAhoB",
    "outputId": "e2fddbf2-f92f-441e-bcfd-9e5b9e962140"
   },
   "outputs": [],
   "source": [
    "#Visually see how classes are distributed across variable combinations:\n",
    "data_plot = pd.DataFrame(X, columns=['Humidity', 'Temperature', 'Vibration', 'Power_Usage'])\n",
    "data_plot['Target'] = y\n",
    "sns.pairplot(data_plot, hue='Target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ovm_8oNGBYv7"
   },
   "source": [
    "Visually, the classes are overlapping, making it difficult to separate them. Therefore, linear methods would not work as expected. And they all follow a normal distribution, as previously presented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOkDydElGjuY"
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "54a6a0fc8f984ae3b612ebcd91ea9de6",
      "8f70ed3446c84559bb6bd498430f7772",
      "07cbc65f2e9b43b4afa552802aaa557d",
      "4c3b83fc9fdc430c9ae1e671dba59c09",
      "215f0aa5f9814e26808337f5d6bc9fe7",
      "86881903ef9d4c41a980e34b9691c4cb",
      "5df44b6c49ae44a880781feb99f7be43",
      "7699e0f1c10d4b0fb99c05adc7b6a453",
      "af3e137b8373495e881858db67031c70",
      "49ddd8fffdd74718aa07bd617bed57c6",
      "aeabf4fe4a3644c5b7066ec5c1d1d3cf"
     ]
    },
    "id": "s73CsVyVQQFd",
    "outputId": "106dbb50-3888-4d7e-9683-db629b7dd9c9"
   },
   "outputs": [],
   "source": [
    "# LazyClassifier\n",
    "clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "\n",
    "# Fit and compare models\n",
    "models, predictions = clf.fit(X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "\n",
    "# Display the results\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R4Pbscj0SxGV",
    "outputId": "bcc41d07-f832-4d4a-de39-d07d8b6f7372"
   },
   "outputs": [],
   "source": [
    "# Top 10 models\n",
    "print(models.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0VwZRxY3xgEg"
   },
   "source": [
    "## Rebalancing classes\n",
    "\n",
    "\n",
    "Before we train the model, we must rebalance the classes, as metrics like ROC AUC and Balanced Accuracy are \"guessing\" values ​​randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qiCBpettyIAU",
    "outputId": "08c69930-5923-44d4-9a9c-c69c6ea43d33"
   },
   "outputs": [],
   "source": [
    "#Rebalancing\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(X_train_resampled)\n",
    "print(y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NdWBUf518RfH"
   },
   "outputs": [],
   "source": [
    "#Scaler again\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "56bcbfbabf874d80a7d21b6a8c6b5cf2",
      "b984070e4df04c4a838edf02bbce2ca8",
      "3a817bd797214f5c9c07b75a9182b15f",
      "a0467799b7a64bda9fb1560d4493b6b3",
      "d9fb4483b99c4bdda7934be8e860263e",
      "e6598b9fe4144a36873f0d7af3fa151a",
      "373133dacd4343679324d4bceac45198",
      "8ac1498e4a8f4ebab6e6b02614ea7d42",
      "20f44e5eb3d64292a74b6b25d4387b15",
      "5194d3d02aa14c2ca60f56376960a925",
      "db6f230423d44cbea7bae467c740edf6"
     ]
    },
    "id": "lWqibgvU9Noy",
    "outputId": "4955350a-0318-468d-9b2f-b92b840fec0d"
   },
   "outputs": [],
   "source": [
    "# LazyClassifier\n",
    "clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "\n",
    "# Fit and compare models with resampled data\n",
    "models_2, predictions_2 = clf.fit(X_train_scaled, X_test_scaled, y_train_resampled, y_test)\n",
    "\n",
    "# Display the results\n",
    "print(models_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T_FGoDOy9x7b",
    "outputId": "cbec6225-846a-430f-fb8c-272411e581e6"
   },
   "outputs": [],
   "source": [
    "# Top 10 models_2\n",
    "print(models_2.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AQs-vI7Y-UaM"
   },
   "source": [
    "Separability between classes is still weak, even with balanced data.\n",
    "\n",
    "→ May indicate overlap between classes or features with little predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hwe2wTL0_bDc"
   },
   "source": [
    "## Test with XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3bT13_gw6HFL",
    "outputId": "c86dc8d4-ca07-4959-a55d-64c347e99c09"
   },
   "outputs": [],
   "source": [
    "# Train XGBoost classifier\n",
    "xgb_model = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_model.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = xgb_model.predict(X_test_scaled)\n",
    "y_prob = xgb_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Evaluation\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "print(f\"\\nROC AUC Score: {roc_auc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OX07_sW9_3Vd"
   },
   "source": [
    "## Test with RadomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xUG92bZlABYy",
    "outputId": "17e5e2f6-6cb4-4024-93e7-8d8e65583a6e"
   },
   "outputs": [],
   "source": [
    "brf = BalancedRandomForestClassifier(random_state=42)\n",
    "brf.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "# Prediction on test data\n",
    "y_pred = brf.predict(X_test_scaled)\n",
    "y_prob = brf.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Evaluation\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "print(f\"\\nROC AUC Score: {roc_auc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XwHRhdmyAXNn"
   },
   "source": [
    "## Adding new data to improve class balance\n",
    "\n",
    "I'll use Autoencode to add some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "74giC5LM4A88"
   },
   "outputs": [],
   "source": [
    "# Select only data with failures\n",
    "df_failures = df[df[\"Failure_Risk\"] == 1]\n",
    "\n",
    "# Only numerical columns\n",
    "features = [\"Humidity\", \"Temperature\", \"Vibration\", \"Power_Usage\",\"Machine_Type\"]\n",
    "X_failure = df_failures[features].values\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_failure_scaled = scaler.fit_transform(X_failure)\n",
    "\n",
    "# Latent space dimension\n",
    "latent_dim = 2\n",
    "input_dim = X_failure_scaled.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nU6_kTj54bwD"
   },
   "outputs": [],
   "source": [
    "# --- Encoder ---\n",
    "inputs = layers.Input(shape=(input_dim,))\n",
    "h = layers.Dense(16, activation='relu')(inputs)\n",
    "z_mean = layers.Dense(latent_dim, name='z_mean')(h)\n",
    "z_log_var = layers.Dense(latent_dim, name='z_log_var')(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zleg0r6W4cbo"
   },
   "outputs": [],
   "source": [
    "# Sampling layer\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = tf.random.normal(shape=tf.shape(z_mean))\n",
    "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "z = layers.Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0e5v-Uch4hrv"
   },
   "outputs": [],
   "source": [
    "# --- Decoder ---\n",
    "decoder_h = layers.Dense(16, activation='relu')\n",
    "decoder_output = layers.Dense(input_dim, activation='sigmoid')\n",
    "h_decoded = decoder_h(z)\n",
    "outputs = decoder_output(h_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8IEmSRZu4kfn"
   },
   "outputs": [],
   "source": [
    "# Define encoder and decoder separately for generation later\n",
    "encoder = models.Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "decoder_input = layers.Input(shape=(latent_dim,))\n",
    "decoder_model = models.Model(decoder_input, decoder_output(decoder_h(decoder_input)), name='decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mD8qJDmX4pG7"
   },
   "outputs": [],
   "source": [
    "# --- VAE as subclassed model ---\n",
    "class VAE(models.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def compile(self, optimizer):\n",
    "        super(VAE, self).compile()\n",
    "        self.optimizer = optimizer\n",
    "        self.total_loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
    "        self.reconstruction_loss_tracker = tf.keras.metrics.Mean(name=\"reconstruction_loss\")\n",
    "        self.kl_loss_tracker = tf.keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.total_loss_tracker, self.reconstruction_loss_tracker, self.kl_loss_tracker]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            data = data[0]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "\n",
    "            # Calculate losses\n",
    "            reconstruction_loss = tf.reduce_mean(tf.reduce_sum(tf.square(data - reconstruction), axis=1))\n",
    "            kl_loss = -0.5 * tf.reduce_mean(tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TM2ESXMQ4vVk",
    "outputId": "1d2a42fc-b0ed-4cb7-ab17-ce4a98fdafd4"
   },
   "outputs": [],
   "source": [
    "# --- Compile and Train ---\n",
    "vae = VAE(encoder, decoder_model)\n",
    "vae.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "vae.fit(X_failure_scaled, epochs=100, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3bej2i7N9Pcn"
   },
   "source": [
    "## Create a new dataset\n",
    "\n",
    "Generate synthetic data + the old dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SN33lzau9gUx",
    "outputId": "c602a11f-a2a6-4b6d-f62b-d6629c21728d"
   },
   "outputs": [],
   "source": [
    "# Generate synthetic failure data\n",
    "z_samples = np.random.normal(size=(300, latent_dim))\n",
    "generated_data = decoder_model.predict(z_samples)\n",
    "generated_data = scaler.inverse_transform(generated_data)\n",
    "\n",
    "# Create new DataFrame for synthetic failures\n",
    "df_synthetic = pd.DataFrame(generated_data, columns=df_failures.drop(\"Failure_Risk\", axis=1).columns)\n",
    "df_synthetic[\"Failure_Risk\"] = 1\n",
    "\n",
    "# Combine with original dataset\n",
    "df_combined = pd.concat([df, df_synthetic], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vlLqy73iRCDL"
   },
   "source": [
    "## Train and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9zRGpH0wRAuC"
   },
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df_combined.drop(\"Failure_Risk\", axis=1)\n",
    "y = df_combined[\"Failure_Risk\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zNjfwsiRROKI"
   },
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RmnWu_LoRSxQ"
   },
   "outputs": [],
   "source": [
    "# Scale features again\n",
    "scaler_final = MinMaxScaler()\n",
    "X_train_scaled = scaler_final.fit_transform(X_train)\n",
    "X_test_scaled = scaler_final.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QRev4bbNRWY4"
   },
   "source": [
    "## Lazy Classifier again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171,
     "referenced_widgets": [
      "407eb900557448ca805d7277d9b4c0eb",
      "c801d51dd1974c00995f81165700d249",
      "fd8c9d5bbc88462ab6344b2b1f395308",
      "48546526b86f4640ab1fd971c0d0d9d2",
      "49bf1873dcac4ace89e163ba4b895b7b",
      "687747490e5646a7b73dd6bc5818e1da",
      "4669f1b33e87426c9f53aea33bb8b735",
      "4e85f65bd60d469183d8fab93ae1c17d",
      "6e293fdaae534c50872bdca832eaf223",
      "200ffc9b7d114ba0afc321b0a7b779b2",
      "8f2b6b5b94f840dbada47fe9062b1b02"
     ]
    },
    "id": "ym_slUgcRYuL",
    "outputId": "1ba42e59-06f9-4855-8694-1c9ab7526f05"
   },
   "outputs": [],
   "source": [
    "# LazyClassifier to compare models\n",
    "clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "models_3, predictions = clf.fit(X_train_scaled, X_test_scaled, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wm1h1brpRgRu",
    "outputId": "eb315e7d-adf9-4c5b-aa3e-2a95e792f4d9"
   },
   "outputs": [],
   "source": [
    "# Display model comparison\n",
    "print(\"Top models after VAE-based balancing:\")\n",
    "print(models_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RP4hbtZVRqW_",
    "outputId": "83dcdb37-5835-4b31-a347-5484caa87ba8"
   },
   "outputs": [],
   "source": [
    "# Top 10 models\n",
    "print(models_3.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yDl4x2tVqc2"
   },
   "source": [
    "## Testing SGDClassifier, Random Forest and Gradient Boosting\n",
    "\n",
    "Using scores like:\n",
    "\n",
    "- accuracy\n",
    "- f1\n",
    "- recall\n",
    "- precision\n",
    "- roc_AUC\n",
    "- Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XYMFiM7bWJvk"
   },
   "outputs": [],
   "source": [
    "# Scorers for evaluation\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'f1': make_scorer(f1_score),\n",
    "    'recall': make_scorer(recall_score),\n",
    "    'precision': make_scorer(precision_score),\n",
    "    'roc_auc': make_scorer(roc_auc_score)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bapIBHUlWbeo"
   },
   "outputs": [],
   "source": [
    "# Initializing the models\n",
    "models = {\n",
    "    'SGDClassifier': SGDClassifier(random_state=42, loss='log_loss', penalty='l2', max_iter=1000),\n",
    "    'RandomForest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    'GradientBoosting': GradientBoostingClassifier(random_state=42, learning_rate=0.1, n_estimators=100),\n",
    "    'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000, multi_class='multinomial', solver='lbfgs')\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aXCwj1KyWeaa"
   },
   "outputs": [],
   "source": [
    "# Stratified cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "871tzeRdWheE",
    "outputId": "025d2c23-1107-4432-8f77-241572251049"
   },
   "outputs": [],
   "source": [
    "# Running the models\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "\n",
    "    pipeline = ImbPipeline(steps=[\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('smote', SMOTE(random_state=42)),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "\n",
    "    results = cross_validate(\n",
    "        pipeline, X, y,\n",
    "        scoring=scoring,\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        return_train_score=False\n",
    "    )\n",
    "\n",
    "    for metric in scoring.keys():\n",
    "        print(f\"{metric.capitalize()}: {np.mean(results['test_' + metric]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTG8AHIwYuKo"
   },
   "source": [
    "## Optimizing hyperparameters\n",
    "\n",
    "I'll choose Random Forest for my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w6JgMlSNZE2x"
   },
   "outputs": [],
   "source": [
    "# Pipeline including scaling and SMOTE\n",
    "pipeline = ImbPipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('classifier', LogisticRegression(\n",
    "        multi_class='multinomial',\n",
    "        solver='lbfgs',\n",
    "        random_state=42\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IHK-46aoauV1"
   },
   "outputs": [],
   "source": [
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'classifier__C': [0.01, 0.1, 1, 10],\n",
    "    'classifier__penalty': ['l2'],\n",
    "    'classifier__solver': ['lbfgs', 'liblinear']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 247
    },
    "id": "lKwYmanAaxtv",
    "outputId": "79add0ea-b276-40d6-8040-c9bd9baf8c9f"
   },
   "outputs": [],
   "source": [
    "# Grid Search with F1 as scoring metric\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='f1', n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZWJyQlC8bWcH",
    "outputId": "246e7ca5-deec-4baf-8a01-3cc119f23dec"
   },
   "outputs": [],
   "source": [
    "# Best hyperparameters\n",
    "print(\"Best Parameters:\")\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0D4RwUO1bcBf",
    "outputId": "0355471d-5f17-41c9-d6f9-5f28b1b2f2f4"
   },
   "outputs": [],
   "source": [
    "# Evaluation on test set\n",
    "print(\"\\nClassification Report on Test Set:\")\n",
    "y_pred = grid_search.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "7bjmGe2pbgEK",
    "outputId": "0a407215-310d-4237-a85e-5efe9f65cfd5"
   },
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"No Failure\", \"Failure\"], yticklabels=[\"No Failure\", \"Failure\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "o4Fcb8krbmwR",
    "outputId": "de7feb50-58d2-44dc-e9b1-184a43a7cdd0"
   },
   "outputs": [],
   "source": [
    "# ROC Curve and AUC\n",
    "y_prob = grid_search.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {roc_auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zl1MhF5EcsdO"
   },
   "source": [
    "## Testing for more recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fbNRbiebcxph"
   },
   "outputs": [],
   "source": [
    "# Probability predictions for class 1 (failure)\n",
    "y_prob = grid_search.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Testing different thresholds\n",
    "thresholds = np.arange(0.1, 0.9, 0.05)\n",
    "recalls = []\n",
    "precisions = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_thresh = (y_prob >= threshold).astype(int)\n",
    "    report = classification_report(y_test, y_pred_thresh, output_dict=True)\n",
    "    recalls.append(report['1']['recall'])\n",
    "    precisions.append(report['1']['precision'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "s3g6rCpNc2eH",
    "outputId": "3c7b5e02-12db-4692-e862-dfb1adcfe877"
   },
   "outputs": [],
   "source": [
    "# Plotting the precision vs recall curve by threshold\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, recalls, label=\"Recall (Failure)\", marker='o')\n",
    "plt.plot(thresholds, precisions, label=\"Precision (Failure)\", marker='x')\n",
    "plt.axvline(x=0.5, color='gray', linestyle='--', label=\"Default Threshold\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Precision and Recall for Failure Class at Different Thresholds\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xn9uZ2zvdMMr"
   },
   "source": [
    "Low threshold (~0.1–0.3):\n",
    "\n",
    "Recall is close to 1 → The model identifies almost all failures.\n",
    "\n",
    "However, precision drops significantly → There are many false positives (raising alarms when it wasn't necessary).\n",
    "\n",
    "High threshold (>0.5):\n",
    "\n",
    "Precision reaches 1 → The model is almost certain when predicting a failure.\n",
    "\n",
    "However, recall drops drastically → It misses many real failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tgIwTZaOdX4u",
    "outputId": "4def0f7b-a73f-42ab-8029-1b31c5408705"
   },
   "outputs": [],
   "source": [
    "# Applying new threshold (e.g., 0.3)\n",
    "new_threshold = 0.5\n",
    "y_pred_adjusted = (y_prob >= new_threshold).astype(int)\n",
    "\n",
    "# New confusion matrix and classification report\n",
    "print(\"Classification Report with Adjusted Threshold:\")\n",
    "print(classification_report(y_test, y_pred_adjusted))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_adjusted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wmhXsfUSdfBJ"
   },
   "outputs": [],
   "source": [
    "# New ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {roc_auc_score(y_test, y_prob):.2f})\")\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PX6rzoOVd9tX"
   },
   "source": [
    "Class 1 (Failure):\n",
    "\n",
    "Recall: 0.77 → The model correctly identifies 77% of actual failure cases, indicating high sensitivity.\n",
    "\n",
    "Precision: 0.57 → As expected with a lower threshold, the precision decreased due to an increase in false positives.\n",
    "\n",
    "Class 0 (No Failure):\n",
    "\n",
    "Recall: 0.51 → Only 51% of the non-failure cases are correctly classified, suggesting a significant drop in specificity.\n",
    "\n",
    "Precision: 0.72 → Despite the lower recall, when the model predicts no failure, there is still a reasonably good chance it is correct.\n",
    "\n",
    "F1-score:\n",
    "\n",
    "Class 1: 0.65\n",
    "\n",
    "Class 0: 0.59\n",
    "This indicates a better balance between precision and recall for the failure class, which is often the priority in risk-sensitive applications"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
