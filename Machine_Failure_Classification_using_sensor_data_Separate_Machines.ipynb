{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8nkaCZNq4uWn"
   },
   "source": [
    "# Machine Failure Classification using sensor data\n",
    "\n",
    "By: Nathália Alvim\n",
    "\n",
    "E-mail: natalvimdesouza@hotmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZjWxSUjgEqk"
   },
   "source": [
    "Dataset is avaible on kaggle.\n",
    "\n",
    "https://www.kaggle.com/datasets/mujtabamatin/dataset-for-machine-failure-detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "draaPnOZfFHh"
   },
   "source": [
    "The dataset has 1000 samples and 6 variables.\n",
    "\n",
    "\n",
    "The dataset consists of the following features:\n",
    "\n",
    "- Temperature (°C): Continuous data representing the temperature at the machine's\n",
    "critical points. Higher temperatures may indicate potential issues due to overheating.\n",
    "\n",
    "- Vibration (Hz): Frequency of machine vibrations. Abnormal vibrations can signal mechanical misalignment, imbalance, or wear.\n",
    "\n",
    "- Power Usage (kW): Power consumption levels of the machine. Spikes in power usage may indicate increased load or potential mechanical issues.\n",
    "\n",
    "- Humidity (%): Environmental humidity around the machine. High humidity levels could affect machine performance and lead to failure over time.\n",
    "\n",
    "- Machine Type: Categorical data indicating the type of machine (e.g., \"Drill\", \"Lathe\", \"Mill\"). Different machine types may have unique failure patterns.\n",
    "\n",
    "Target Variable:\n",
    "\n",
    "- Failure Risk: A binary label where 0 indicates normal operation, and 1 indicates that the machine is at risk of failure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k1hDRA9n4r3H"
   },
   "source": [
    "##Importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Phz1OGAqe3-3"
   },
   "outputs": [],
   "source": [
    "#Import packages for preprocessing\n",
    "!pip install statsmodels\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "rQX_x1NPgzCE",
    "outputId": "e3f6e04f-7337-45a2-fc25-170db3105e6f"
   },
   "outputs": [],
   "source": [
    "#Loading the dataset\n",
    "file_path = '/workspaces/Machine-Learning/machine_failure_dataset.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KJmcjvEVCRTJ",
    "outputId": "4e45f3b0-bba7-46cd-e4a0-dc4474f41620"
   },
   "outputs": [],
   "source": [
    "# General information\n",
    "print(df.describe())\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JUi7KQ5w4-3S"
   },
   "source": [
    "## Class Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zLylVDFWi55S",
    "outputId": "ce52acbb-9afd-4d01-f810-1bd312ada153"
   },
   "outputs": [],
   "source": [
    "# Class Balancing\n",
    "# First i'll apply for the categorical variables and after i'll do with the binary variable\n",
    "\n",
    "categorical = 'Machine_Type'\n",
    "\n",
    "#Available categories\n",
    "\n",
    "categories = df[categorical].unique()\n",
    "\n",
    "#Counting\n",
    "counting = df[categorical].value_counts()\n",
    "\n",
    "print(categories)\n",
    "print(counting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Jr-Vjii3dSx",
    "outputId": "fec8521d-0bc0-4ee2-ffd1-b2934056537e"
   },
   "outputs": [],
   "source": [
    "#Binary variable\n",
    "\n",
    "binary = 'Failure_Risk'\n",
    "\n",
    "#Available categories\n",
    "\n",
    "categories = df[binary].unique() # 0 = Normal operation \\\\ 1 = Risk of failure\n",
    "\n",
    "#Counting\n",
    "counting = df[binary].value_counts()\n",
    "\n",
    "print(categories)\n",
    "print(counting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y6MJcURLfHmp",
    "outputId": "b2669c42-d01b-4578-962a-1e3644528091"
   },
   "outputs": [],
   "source": [
    "# Machine Type and Failure Risk - number of occurrences\n",
    "summary = df.groupby(['Machine_Type', 'Failure_Risk']).size().unstack(fill_value=0)\n",
    "\n",
    "# Rename columns for clarity\n",
    "summary.columns = ['Normal (0)', 'Failure (1)']\n",
    "\n",
    "# Display the summary table\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJSNeslp4pim"
   },
   "source": [
    "## Descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fsM0SFPy4ZRO",
    "outputId": "5641e08e-7a8c-4265-db7d-577c9e2c6f8b"
   },
   "outputs": [],
   "source": [
    "# Calculate mean, median, and mode for each numerical column\n",
    "# Select only numerical columns (nc) and exclude the binary variable\n",
    "# Select only numerical columns and exclude a specific one\n",
    "nc = df.select_dtypes(include=['number']).drop(columns=['Failure_Risk'])\n",
    "\n",
    "mean = nc.mean()\n",
    "median = nc.median()\n",
    "mode = nc.mode().iloc[0]  # Takes the first mode found for each column\n",
    "std = nc.std()  # Standard deviation\n",
    "cv = std / mean  # Coefficient of Variation\n",
    "\n",
    "\n",
    "print(\"\\nMean of each column:\")\n",
    "print(mean)\n",
    "\n",
    "print(\"\\nMedian of each column:\")\n",
    "print(median)\n",
    "\n",
    "print(\"\\nMode of each column:\")\n",
    "print(mode)\n",
    "\n",
    "print(\"\\nStandard Deviation of each column:\")\n",
    "print(std)\n",
    "\n",
    "print(\"\\nCoefficient of Variation (CV) of each column:\")\n",
    "print(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "ckl6Jl3K59nB",
    "outputId": "8c7ed159-c3ec-4c26-8daa-41f6bcbc9105"
   },
   "outputs": [],
   "source": [
    "# Boxpot\n",
    "# Generate boxplot to visualize distribution and possible outliers\n",
    "plt.figure(figsize=(7, 5))\n",
    "nc.boxplot()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "OPHOKN5m6fTC",
    "outputId": "4412e6fd-3ceb-4356-9290-be48857e98e8"
   },
   "outputs": [],
   "source": [
    "# Generate paired boxplots to save space and for better visualization\n",
    "columns = nc.columns\n",
    "num_columns = len(columns)\n",
    "\n",
    "# Iterate over the columns in pairs\n",
    "for i in range(0, num_columns, 2):\n",
    "    # Set up the subplot for 2 side-by-side plots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # First boxplot\n",
    "    axes[0].boxplot(nc[columns[i]].dropna(), vert=True, patch_artist=True)\n",
    "    axes[0].set_title(f\"Boxplot - {columns[i]}\")\n",
    "    axes[0].set_xlabel(columns[i])\n",
    "    axes[0].set_ylabel(\"Values\")\n",
    "\n",
    "    # Second boxplot (if it exists)\n",
    "    if i + 1 < num_columns:\n",
    "        axes[1].boxplot(nc[columns[i + 1]].dropna(), vert=True, patch_artist=True)\n",
    "        axes[1].set_title(f\"Boxplot - {columns[i + 1]}\")\n",
    "        axes[1].set_xlabel(columns[i + 1])\n",
    "        axes[1].set_ylabel(\"Values\")\n",
    "    else:\n",
    "        # Remove the second subplot if there's no second column\n",
    "        fig.delaxes(axes[1])\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYPvGNHT79W_"
   },
   "source": [
    "## Normality of the dataset\n",
    "\n",
    "- Shapiro-Wilk\n",
    "\n",
    "- D’Agostino-Pearson\n",
    "\n",
    "- Anderson-Darling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ygNHuo4g8on6"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import shapiro, normaltest, anderson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0YHEYnK2779j"
   },
   "outputs": [],
   "source": [
    "def normality_tests_summary(dataframe):\n",
    "    numeric_cols = dataframe.select_dtypes(include=['number'])\n",
    "    results = []\n",
    "\n",
    "    for col in numeric_cols.columns:\n",
    "        col_data = numeric_cols[col].dropna()\n",
    "\n",
    "        # Shapiro-Wilk Test\n",
    "        shapiro_stat, shapiro_p = shapiro(col_data)\n",
    "        shapiro_result = \"Normal\" if shapiro_p > 0.05 else \"Not normal\"\n",
    "\n",
    "        # D’Agostino and Pearson Test\n",
    "        dagostino_stat, dagostino_p = normaltest(col_data)\n",
    "        dagostino_result = \"Normal\" if dagostino_p > 0.05 else \"Not normal\"\n",
    "\n",
    "        # Anderson-Darling Test\n",
    "        anderson_result_obj = anderson(col_data)\n",
    "        ad_stat = anderson_result_obj.statistic\n",
    "        # Compare with 5% significance level\n",
    "        ad_result = \"Normal\" if ad_stat < anderson_result_obj.critical_values[2] else \"Not normal\"\n",
    "\n",
    "        results.append({\n",
    "            \"Column\": col,\n",
    "            \"Shapiro-Wilk (p)\": round(shapiro_p, 4),\n",
    "            \"Shapiro Result\": shapiro_result,\n",
    "            \"D’Agostino (p)\": round(dagostino_p, 4),\n",
    "            \"D’Agostino Result\": dagostino_result,\n",
    "            \"Anderson-Darling (stat)\": round(ad_stat, 4),\n",
    "            \"Anderson Result (5%)\": ad_result\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cf51DMkE8t69",
    "outputId": "2e6223fd-7129-4d7e-a23a-45c75349005d"
   },
   "outputs": [],
   "source": [
    "summary_df = normality_tests_summary(nc)\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFjtPiyU6tyr"
   },
   "source": [
    "## Pearson correlation map\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 682
    },
    "id": "rSlHbRGm6q1e",
    "outputId": "e8300f46-a906-4815-b795-d8326ac7e8e8"
   },
   "outputs": [],
   "source": [
    "# Pearson Correlation Map\n",
    "# Calculate the correlation matrix (Pearson coefficient)\n",
    "pearson_correlation = nc.corr(method='pearson')\n",
    "\n",
    "# Generate the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(pearson_correlation, annot=True, cmap=\"coolwarm\", fmt=\".2f\", cbar=True, square=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pwMTXoPxJebh"
   },
   "source": [
    "### Conclusion:\n",
    "\n",
    "The data set follows a normal distribution by tests performed such as Shapiro Wilk.\n",
    "A graph was generated in order to define the linearity of the system, thus concluding that the system is non-linear.\n",
    "Finally, the Pearson correlation map was created to see the relationship between the variables.\n",
    "\n",
    "In summary:\n",
    "\n",
    "-> Non-linear\n",
    "\n",
    "-> Normal Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IB2_i6-ODTZc"
   },
   "source": [
    "## Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jsucHeTFFsxx"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E0E2q_ySDWg8"
   },
   "outputs": [],
   "source": [
    "# Separate the dataset\n",
    "drill_df = df[df['Machine_Type'] == 'Drill'].copy()\n",
    "lathe_df = df[df['Machine_Type'] == 'Lathe'].copy()\n",
    "mill_df = df[df['Machine_Type'] == 'Mill'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NwC0me9GkkP7"
   },
   "outputs": [],
   "source": [
    "# Separate features (X) and target (y)\n",
    "X = drill_df.drop(columns=['Failure_Risk', 'Machine_Type'])\n",
    "y = drill_df['Failure_Risk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WCtFpSItFxSh"
   },
   "outputs": [],
   "source": [
    "#80 training and 20 for test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "gme0h0iNl1aU",
    "outputId": "6573fd68-e729-43b6-fce3-0b92029c840b"
   },
   "outputs": [],
   "source": [
    "# Visually see how classes are distributed across variable combinations for Machine Type - Drill\n",
    "X = drill_df.drop(columns=['Failure_Risk', 'Machine_Type'])\n",
    "y = drill_df['Failure_Risk']\n",
    "\n",
    "data_plot = pd.DataFrame(X, columns=['Humidity', 'Temperature', 'Vibration', 'Power_Usage'])\n",
    "data_plot['Target'] = y\n",
    "\n",
    "# Pairplot\n",
    "sns.pairplot(data_plot, hue='Target')\n",
    "plt.suptitle('Drill - Distribuição das Variáveis por Risco de Falha', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "I7UzQlDgmR71",
    "outputId": "5081d4e6-6a5f-4c26-a6e8-6009d76cddde"
   },
   "outputs": [],
   "source": [
    "# Visually see how classes are distributed across variable combinations for Machine Type - Lathe\n",
    "X = lathe_df.drop(columns=['Failure_Risk', 'Machine_Type'])\n",
    "y = lathe_df['Failure_Risk']\n",
    "\n",
    "data_plot = pd.DataFrame(X, columns=['Humidity', 'Temperature', 'Vibration', 'Power_Usage'])\n",
    "data_plot['Target'] = y\n",
    "\n",
    "# Pairplot\n",
    "sns.pairplot(data_plot, hue='Target')\n",
    "plt.suptitle('Lathe- Feature Distribution by Failure Risk', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "MQeBHFTNmSwI",
    "outputId": "e37a380f-53f2-40ef-ef27-f20ffc4f97c7"
   },
   "outputs": [],
   "source": [
    "# Visually see how classes are distributed across variable combinations for Machine Type - Mill\n",
    "X = mill_df.drop(columns=['Failure_Risk', 'Machine_Type'])\n",
    "y = mill_df['Failure_Risk']\n",
    "data_plot = pd.DataFrame(X, columns=['Humidity', 'Temperature', 'Vibration', 'Power_Usage'])\n",
    "data_plot['Target'] = y\n",
    "\n",
    "# Pairplot\n",
    "sns.pairplot(data_plot, hue='Target')\n",
    "plt.suptitle('Mill - Feature Distribution by Failure Risk', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XwHRhdmyAXNn"
   },
   "source": [
    "## Adding new data to improve class balance\n",
    "\n",
    "I'll use Autoencode to add some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fZzFhtO_4M7V"
   },
   "outputs": [],
   "source": [
    "#Import packages tensor flow\n",
    "!pip install tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "74giC5LM4A88"
   },
   "outputs": [],
   "source": [
    "# Select only data with failures\n",
    "df_failures = df[df[\"Failure_Risk\"] == 1]\n",
    "\n",
    "# Only numerical columns\n",
    "features = [\"Humidity\", \"Temperature\", \"Vibration\", \"Power_Usage\"]\n",
    "X_failure = df_failures[features].values\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_failure_scaled = scaler.fit_transform(X_failure)\n",
    "\n",
    "# Latent space dimension\n",
    "latent_dim = 2\n",
    "input_dim = X_failure_scaled.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nU6_kTj54bwD"
   },
   "outputs": [],
   "source": [
    "def build_vae(input_dim, latent_dim=2):\n",
    "# Encoder\n",
    "    inputs = layers.Input(shape=(input_dim,))\n",
    "    h = layers.Dense(16, activation='relu')(inputs)\n",
    "    z_mean = layers.Dense(latent_dim, name='z_mean')(h)\n",
    "    z_log_var = layers.Dense(latent_dim, name='z_log_var')(h)\n",
    "\n",
    "    # Sampling layer\n",
    "    def sampling(args):\n",
    "        z_mean, z_log_var = args\n",
    "        epsilon = tf.random.normal(shape=tf.shape(z_mean))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "    z = layers.Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "    # Decoder\n",
    "    decoder_h = layers.Dense(16, activation='relu')\n",
    "    decoder_output = layers.Dense(input_dim, activation='sigmoid')\n",
    "    h_decoded = decoder_h(z)\n",
    "    outputs = decoder_output(h_decoded)\n",
    "\n",
    "    encoder = models.Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "    decoder_input = layers.Input(shape=(latent_dim,))\n",
    "    decoder_model = models.Model(decoder_input, decoder_output(decoder_h(decoder_input)), name='decoder')\n",
    "\n",
    "    # VAE\n",
    "    class VAE(models.Model):\n",
    "        def __init__(self, encoder, decoder, **kwargs):\n",
    "            super(VAE, self).__init__(**kwargs)\n",
    "            self.encoder = encoder\n",
    "            self.decoder = decoder\n",
    "\n",
    "        def compile(self, optimizer):\n",
    "            super(VAE, self).compile()\n",
    "            self.optimizer = optimizer\n",
    "            self.total_loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
    "            self.reconstruction_loss_tracker = tf.keras.metrics.Mean(name=\"reconstruction_loss\")\n",
    "            self.kl_loss_tracker = tf.keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "        @property\n",
    "        def metrics(self):\n",
    "            return [self.total_loss_tracker, self.reconstruction_loss_tracker, self.kl_loss_tracker]\n",
    "\n",
    "        def train_step(self, data):\n",
    "            if isinstance(data, tuple):\n",
    "                data = data[0]\n",
    "            with tf.GradientTape() as tape:\n",
    "                z_mean, z_log_var, z = self.encoder(data)\n",
    "                reconstruction = self.decoder(z)\n",
    "                reconstruction_loss = tf.reduce_mean(tf.reduce_sum(tf.square(data - reconstruction), axis=1))\n",
    "                kl_loss = -0.5 * tf.reduce_mean(tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1))\n",
    "                total_loss = reconstruction_loss + kl_loss\n",
    "\n",
    "            grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "            self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "\n",
    "            self.total_loss_tracker.update_state(total_loss)\n",
    "            self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "            self.kl_loss_tracker.update_state(kl_loss)\n",
    "            return {\n",
    "                \"loss\": self.total_loss_tracker.result(),\n",
    "                \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "                \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "            }\n",
    "\n",
    "    return VAE(encoder, decoder_model), encoder, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TM2ESXMQ4vVk"
   },
   "outputs": [],
   "source": [
    "def generate_synthetic_failures(df_machine, features, samples_to_generate=300):\n",
    "    df_failures = df_machine[df_machine[\"Failure_Risk\"] == 1].copy()\n",
    "    scaler = MinMaxScaler()\n",
    "    X_failure_scaled = scaler.fit_transform(df_failures[features])\n",
    "\n",
    "    vae, encoder, decoder = build_vae(input_dim=X_failure_scaled.shape[1])\n",
    "    vae.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "    vae.fit(X_failure_scaled, epochs=100, batch_size=32, verbose=0)\n",
    "\n",
    "    z_samples = np.random.normal(size=(samples_to_generate, 2))\n",
    "    generated_data = decoder.predict(z_samples)\n",
    "    generated_data = scaler.inverse_transform(generated_data)\n",
    "\n",
    "    df_synthetic = pd.DataFrame(generated_data, columns=features)\n",
    "    df_synthetic[\"Failure_Risk\"] = 1\n",
    "    df_synthetic[\"Machine_Type\"] = df_machine[\"Machine_Type\"].iloc[0]  # preserve machine label\n",
    "\n",
    "    return df_synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z87rzbx2vIeQ",
    "outputId": "a9a95ab3-6689-4737-9283-b65b3a7715c9"
   },
   "outputs": [],
   "source": [
    "# --- APPLY TO ALL MACHINE TYPES ---\n",
    "features = [\"Humidity\", \"Temperature\", \"Vibration\", \"Power_Usage\"]\n",
    "\n",
    "df_drill = df[df[\"Machine_Type\"] == \"Drill\"].copy()\n",
    "df_lathe = df[df[\"Machine_Type\"] == \"Lathe\"].copy()\n",
    "df_mill  = df[df[\"Machine_Type\"] == \"Mill\"].copy()\n",
    "\n",
    "df_synth_drill = generate_synthetic_failures(df_drill, features)\n",
    "df_synth_lathe = generate_synthetic_failures(df_lathe, features)\n",
    "df_synth_mill  = generate_synthetic_failures(df_mill, features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3bej2i7N9Pcn"
   },
   "source": [
    "## Create a new dataset\n",
    "\n",
    "Generate synthetic data + the old dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SN33lzau9gUx"
   },
   "outputs": [],
   "source": [
    "# Combine with original data\n",
    "df_drill_aug = pd.concat([df_drill, df_synth_drill], ignore_index=True)\n",
    "df_lathe_aug = pd.concat([df_lathe, df_synth_lathe], ignore_index=True)\n",
    "df_mill_aug  = pd.concat([df_mill, df_synth_mill], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vlLqy73iRCDL"
   },
   "source": [
    "## Train and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G5XNUTwwvcoo"
   },
   "outputs": [],
   "source": [
    "# From here, do the usual train-test split for Drill:\n",
    "X_drill = df_drill_aug[features]\n",
    "y_drill = df_drill_aug[\"Failure_Risk\"]\n",
    "X_train_drill, X_test_drill, y_train_drill, y_test_drill = train_test_split(X_drill, y_drill, test_size=0.3, stratify=y_drill, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M8ejYNAGve2c"
   },
   "outputs": [],
   "source": [
    "# From here, do the usual train-test split for Lathe:\n",
    "X_lathe = df_lathe_aug[features]\n",
    "y_lathe = df_lathe_aug[\"Failure_Risk\"]\n",
    "X_train_lathe, X_test_lathe, y_train_lathe, y_test_lathe = train_test_split(X_lathe, y_lathe, test_size=0.3, stratify=y_lathe, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e_Ik9VU3vf1f"
   },
   "outputs": [],
   "source": [
    "# From here, do the usual train-test split for Mill:\n",
    "X_mill = df_mill_aug[features]\n",
    "y_mill = df_mill_aug[\"Failure_Risk\"]\n",
    "X_train_mill, X_test_mill, y_train_mill, y_test_mill = train_test_split(X_mill, y_mill, test_size=0.3, stratify=y_mill, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RmnWu_LoRSxQ"
   },
   "outputs": [],
   "source": [
    "# Scale features again\n",
    "# --- Drill ---\n",
    "scaler_drill = MinMaxScaler()\n",
    "X_train_drill_scaled = scaler_drill.fit_transform(X_train_drill)\n",
    "X_test_drill_scaled = scaler_drill.transform(X_test_drill)\n",
    "\n",
    "# --- Lathe ---\n",
    "scaler_lathe = MinMaxScaler()\n",
    "X_train_lathe_scaled = scaler_lathe.fit_transform(X_train_lathe)\n",
    "X_test_lathe_scaled = scaler_lathe.transform(X_test_lathe)\n",
    "\n",
    "# --- Mill ---\n",
    "scaler_mill = MinMaxScaler()\n",
    "X_train_mill_scaled = scaler_mill.fit_transform(X_train_mill)\n",
    "X_test_mill_scaled = scaler_mill.transform(X_test_mill)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOkDydElGjuY"
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RrDQ8ZiMPpLM",
    "outputId": "27022be1-9b2b-4bfa-cf36-ce13e6be2f8f"
   },
   "outputs": [],
   "source": [
    "#Installing Lazy Predict\n",
    "!pip install lazypredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9FzCg9VwGjG-"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, cross_validate\n",
    "from lazypredict.Supervised import LazyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "c5172310d7cc4939995dd8b977b973f0",
      "1d9e4985b4a9473580e1be394cb56c61",
      "1fdb8b9bd4ab4dc9af45de2ccee1ebe0",
      "c588bd74b3fa43f59904fa4ce4e518ed",
      "1e8eb86896224b8bbca42f4045512aa4",
      "d8a1becbca6d4eedb358c76a6959e416",
      "7a0abf032f6146598e8b1de48fdf721e",
      "6179fc93566b4549836724dbaf15b7f2",
      "41eaf7ea49244e33bf06e210664a5fa1",
      "4e2cb923c6db457887fc818ba92b2a76",
      "3c436d4cfab74cd7912fef9896961b33",
      "f98d752f66af4f29af90bacaed1bc323",
      "d263259615cc48fd803c227037ec3c20",
      "db5c35077a4f43fab5171828379734f2",
      "868f433b237a497a95eec1d2778443c7",
      "edf41715d8104b0982c5e16721176000",
      "8ce96377938f46fab91e6ce84ac96cf0",
      "5f328fd163364311bc2173fc092f0058",
      "ef4bc90191814577abe7783c6c85aab9",
      "df009a607fa541b28728b5fff44adda9",
      "ab9b324907ce4d138cb661e73ec02615",
      "8628ad00dd3743798ac90e2e4f604b14",
      "8b966ac1ba3b491cb59f1e60d9a23e9e",
      "80b2fb2ec3514b1ca87f35dcc34c5185",
      "daf0fd9bd71742fabbb6ab5e0af4922a",
      "6067a35a6c02426f94a9c3a129cbd341",
      "69a62c22d0fc46ec8f28910c140e0260",
      "966288f875fc4fef97469ee68bb43db6",
      "b840144dd81d4248a5632aad0b7c593d",
      "15a6cfa8156e4bf19e27527a4a2e62e2",
      "ab9aae160468449dade74e84dd10e357",
      "3c56e57987094e77b584fc4ee7170f7b",
      "e8c24561453c4c458693e75e9b5ad322"
     ]
    },
    "id": "s73CsVyVQQFd",
    "outputId": "bbebb87b-2e3c-4ba0-94bb-549551132abd"
   },
   "outputs": [],
   "source": [
    "# Run LazyClassifier for all machine types:\n",
    "\n",
    "# --- Drill ---\n",
    "clf_drill = LazyClassifier(verbose=0, ignore_warnings=True, predictions=False, custom_metric=None)\n",
    "models_drill, predictions_drill = clf_drill.fit(X_train_drill_scaled, X_test_drill_scaled, y_train_drill, y_test_drill)\n",
    "print(\"Drill Results:\\n\", models_drill)\n",
    "\n",
    "# --- Lathe ---\n",
    "clf_lathe = LazyClassifier(verbose=0, ignore_warnings=True)\n",
    "models_lathe, predictions_lathe = clf_lathe.fit(X_train_lathe_scaled, X_test_lathe_scaled, y_train_lathe, y_test_lathe)\n",
    "print(\"\\nLathe Results:\\n\", models_lathe)\n",
    "\n",
    "# --- Mill ---\n",
    "clf_mill = LazyClassifier(verbose=0, ignore_warnings=True)\n",
    "models_mill, predictions_mill = clf_mill.fit(X_train_mill_scaled, X_test_mill_scaled, y_train_mill, y_test_mill)\n",
    "print(\"\\nMill Results:\\n\", models_mill)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jK1Z99NA7Xw8",
    "outputId": "9d469b9f-8fdb-41ce-df26-f46769736355"
   },
   "outputs": [],
   "source": [
    "# Default Lazy Classifier\n",
    "for model_name, model_object in clf_drill.models.items():\n",
    "    print(f\"\\nModel Drill: {model_name}\")\n",
    "    print(model_object.get_params())\n",
    "\n",
    "for model_name, model_object in clf_lathe.models.items():\n",
    "    print(f\"\\nModel Lathe: {model_name}\")\n",
    "    print(model_object.get_params())\n",
    "\n",
    "for model_name, model_object in clf_mill.models.items():\n",
    "    print(f\"\\nModel Mill: {model_name}\")\n",
    "    print(model_object.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RLZ-__b-yA6I"
   },
   "source": [
    "###  Resultados para Drill\n",
    "\n",
    "| Modelo                | Acc  | Bal. Acc | ROC AUC | F1   |\n",
    "|-----------------------|------|----------|---------|------|\n",
    "| NuSVC                 | 0.84 | 0.86     | 0.86    | 0.85 |\n",
    "| SVC                   | 0.83 | 0.84     | 0.84    | 0.83 |\n",
    "| BaggingClassifier     | 0.83 | 0.84     | 0.84    | 0.83 |\n",
    "| KNeighborsClassifier  | 0.83 | 0.83     | 0.83    | 0.83 |\n",
    "| ExtraTreesClassifier  | 0.83 | 0.82     | 0.82    | 0.83 |\n",
    "\n",
    "---\n",
    "\n",
    "###  Resultados para Lathe\n",
    "\n",
    "| Modelo                | Acc  | Bal. Acc | ROC AUC | F1   |\n",
    "|-----------------------|------|----------|---------|------|\n",
    "| NuSVC                 | 0.87 | 0.88     | 0.88    | 0.87 |\n",
    "| SVC                   | 0.86 | 0.87     | 0.87    | 0.87 |\n",
    "| BaggingClassifier     | 0.85 | 0.84     | 0.84    | 0.85 |\n",
    "| XGBClassifier         | 0.85 | 0.84     | 0.84    | 0.85 |\n",
    "| RandomForestClassifier| 0.84 | 0.84     | 0.84    | 0.84 |\n",
    "\n",
    "---\n",
    "\n",
    "###  Resultados para Mill\n",
    "\n",
    "| Modelo                | Acc  | Bal. Acc | ROC AUC | F1   |\n",
    "|-----------------------|------|----------|---------|------|\n",
    "| ExtraTreesClassifier  | 0.87 | 0.87     | 0.87    | 0.87 |\n",
    "| BaggingClassifier     | 0.87 | 0.87     | 0.87    | 0.87 |\n",
    "| SVC                   | 0.86 | 0.86     | 0.86    | 0.86 |\n",
    "| NuSVC                 | 0.84 | 0.83     | 0.83    | 0.84 |\n",
    "| RandomForestClassifier| 0.84 | 0.83     | 0.83    | 0.84 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yDl4x2tVqc2"
   },
   "source": [
    "## Testing for all machines\n",
    "\n",
    "Models:\n",
    "\n",
    "- SVC ( Drill and Lathe)\n",
    "\n",
    "- Extra Trees Classifier (Mill)\n",
    "\n",
    "Using scores like:\n",
    "\n",
    "- accuracy\n",
    "- f1\n",
    "- recall\n",
    "- precision\n",
    "- roc_AUC\n",
    "- Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m3HlWK2BVp5U"
   },
   "outputs": [],
   "source": [
    "!pip install imblearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score, recall_score, precision_score,roc_auc_score\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0yxLb3hTAsWv"
   },
   "outputs": [],
   "source": [
    "# Scoring metrics\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'f1': make_scorer(f1_score),\n",
    "    'recall': make_scorer(recall_score),\n",
    "    'precision': make_scorer(precision_score),\n",
    "    'roc_auc': make_scorer(roc_auc_score)\n",
    "}\n",
    "\n",
    "# Stratified CV\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h1UJ43Mb9aJ9",
    "outputId": "8fc2f51c-d6d7-4311-a52a-e5e744d08107"
   },
   "outputs": [],
   "source": [
    "# --- Drill Dataset ---\n",
    "X_drill = df_drill_aug.drop(['Failure_Risk', 'Machine_Type'], axis=1)\n",
    "y_drill = df_drill_aug['Failure_Risk']\n",
    "\n",
    "# LazyClassifier with SVC\n",
    "pipeline_drill = ImbPipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('classifier', SVC(\n",
    "        kernel='rbf',\n",
    "        gamma='scale',\n",
    "        probability=True,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Cross-validation\n",
    "results_drill = cross_validate(\n",
    "    pipeline_drill,\n",
    "    X_drill,\n",
    "    y_drill,\n",
    "    scoring=scoring,\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    return_train_score=False,\n",
    "    error_score='raise'\n",
    ")\n",
    "\n",
    "# Mean Results\n",
    "print(\"\\n--- Drill (SVC) ---\")\n",
    "for metric in scoring:\n",
    "    print(f\"{metric.capitalize()}: {np.mean(results_drill['test_' + metric]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dojKGJZtCS61",
    "outputId": "06263bfa-76f0-41d0-ee92-b1e40c2eff4c"
   },
   "outputs": [],
   "source": [
    "# --- Lathe Dataset ---\n",
    "X_lathe = df_lathe_aug.drop(['Failure_Risk', 'Machine_Type'], axis=1)\n",
    "y_lathe = df_lathe_aug['Failure_Risk']\n",
    "\n",
    "# LazyClassifier with SVC\n",
    "pipeline_lathe = ImbPipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('classifier', SVC(\n",
    "        kernel='rbf',\n",
    "        gamma='scale',\n",
    "        probability=True,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Cross-validation\n",
    "results_lathe = cross_validate(\n",
    "    pipeline_lathe,\n",
    "    X_lathe,\n",
    "    y_lathe,\n",
    "    scoring=scoring,\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    return_train_score=False,\n",
    "    error_score='raise'\n",
    ")\n",
    "\n",
    "# Mean Results\n",
    "print(\"\\n--- Lathe (SVC) ---\")\n",
    "for metric in scoring:\n",
    "    print(f\"{metric.capitalize()}: {np.mean(results_lathe['test_' + metric]):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6zIRv8Z5EYy0",
    "outputId": "90ec7f14-3b7a-4384-d6b0-d8e964225676"
   },
   "outputs": [],
   "source": [
    "# --- Mill Dataset ---\n",
    "X_mill = df_mill_aug.drop(['Failure_Risk', 'Machine_Type'], axis=1)\n",
    "y_mill = df_mill_aug['Failure_Risk']\n",
    "\n",
    "# LazyClassifier com ExtraTreesClassifier\n",
    "pipeline_mill = ImbPipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('classifier', ExtraTreesClassifier(\n",
    "        n_estimators=100,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Cross-validation\n",
    "results_mill = cross_validate(\n",
    "    pipeline_mill,\n",
    "    X_mill,\n",
    "    y_mill,\n",
    "    scoring=scoring,\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    return_train_score=False,\n",
    "    error_score='raise'\n",
    ")\n",
    "\n",
    "# Mean Results\n",
    "print(\"\\n--- Mill (ExtraTreesClassifier) ---\")\n",
    "for metric in scoring:\n",
    "    print(f\"{metric.capitalize()}: {np.mean(results_mill['test_' + metric]):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_xcZ4dwF5wQ"
   },
   "source": [
    "| Modelo           | Accuracy | F1 Score | Recall | Precision | ROC AUC |\n",
    "|------------------|----------|----------|--------|-----------|---------|\n",
    "| Drill (SVC)    | 0.8122   | 0.8379   | 0.7697 | 0.9227    | 0.8392  |\n",
    "| Lathe (SVC)    | 0.8182   | 0.8423   | 0.7679 | 0.9365    | 0.8408  |\n",
    "| Mill (ExtraTrees)| 0.8296   | 0.8509   | 0.7827 | 0.9547    | 0.8451  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KkxOUQhYGQsT"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "from sklearn.model_selection import cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1unuF7DNGXdt"
   },
   "outputs": [],
   "source": [
    "# Function to plot confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, title=\"Confusion Matrix\"):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "                xticklabels=['Normal', 'Failure'], yticklabels=['Normal', 'Failure'])\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Prediction')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.show()\n",
    "\n",
    "# Function to plot ROC curve and AUC\n",
    "def plot_roc_curve(y_true, y_prob, title=\"ROC Curve\"):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='AUC = %0.2f' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Taxa de Falsos Positivos')\n",
    "    plt.ylabel('Txa de Verdadeiros Positivos')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "g8kP9LPjGiUS",
    "outputId": "97ad720b-fa6f-4995-ab17-56411a0adef3"
   },
   "outputs": [],
   "source": [
    "# --- Drill (SVC) ---\n",
    "y_pred_cv = cross_val_predict(pipeline_drill, X_drill, y_drill, cv=cv, method='predict')\n",
    "y_prob_cv = cross_val_predict(pipeline_drill, X_drill, y_drill, cv=cv, method='predict_proba')[:, 1]\n",
    "\n",
    "# Confusion Matrix and ROC Curve for Drill\n",
    "plot_confusion_matrix(y_drill, y_pred_cv, title=\"Confusion Matrix (Cross-Validation)\")\n",
    "plot_roc_curve(y_drill, y_prob_cv, title=\"Curva ROC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "RwtEnhUmIKnb",
    "outputId": "b41f6fa2-0ca9-4994-a0d3-0109f9d7f450"
   },
   "outputs": [],
   "source": [
    "# --- Lathe (NuSVC) ---\n",
    "y_lathe_pred = cross_val_predict(pipeline_lathe, X_lathe, y_lathe, cv=cv, method='predict')\n",
    "y_lathe_prob = cross_val_predict(pipeline_lathe, X_lathe, y_lathe, cv=cv, method='predict_proba')[:, 1]\n",
    "\n",
    "# Confusion Matrix and ROC Curve for Lathe\n",
    "plot_confusion_matrix(y_lathe, y_lathe_pred, title=\"Confusion Matrix - Lathe (NuSVC)\")\n",
    "plot_roc_curve(y_lathe, y_lathe_prob, title=\"Curva ROC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "nyDj4-w0IQHz",
    "outputId": "12f3a770-7c7d-4620-bf47-dc9641ace0ca"
   },
   "outputs": [],
   "source": [
    "# --- Mill (ExtraTreesClassifier) ---\n",
    "y_mill_pred = cross_val_predict(pipeline_mill, X_mill, y_mill, cv=cv, method='predict')\n",
    "y_mill_prob = cross_val_predict(pipeline_mill, X_mill, y_mill, cv=cv, method='predict_proba')[:, 1]\n",
    "\n",
    "# Confusion Matrix and ROC Curve for Mill\n",
    "plot_confusion_matrix(y_mill, y_mill_pred, title=\"Confusion Matrix - Mill (ExtraTreesClassifier)\")\n",
    "plot_roc_curve(y_mill, y_mill_prob, title=\"Curva ROC\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
